/**
 * QA Engineer Agent
 * Tests features and ensures quality
 */

import type { Env, Task, AgentId } from '../types';
import { Logger } from '../utils/logger';

export interface TestCase {
  id: string;
  title: string;
  description: string;
  pre_conditions: string[];
  steps: string[];
  expected_result: string;
  actual_result?: string;
  status: 'pending' | 'passed' | 'failed' | 'skipped';
  severity: 'critical' | 'high' | 'medium' | 'low';
  created_at: number;
}

export interface BugReport {
  id: string;
  title: string;
  severity: 'critical' | 'high' | 'medium' | 'low';
  steps_to_reproduce: string[];
  expected_behavior: string;
  actual_behavior: string;
  environment: string;
  screenshots?: string[];
  logs?: string;
  task_id: string;
  created_at: number;
}

export class QAEngineerAgent {
  private logger: Logger;
  private agentId: AgentId = 'agent-qa';

  constructor(private env: Env) {
    this.logger = new Logger(env, 'QAEngineerAgent');
  }

  /**
   * Process QA task - test a feature
   */
  async processTask(task: Task): Promise<{
    test_plan: string;
    test_cases: TestCase[];
    test_results: {
      total: number;
      passed: number;
      failed: number;
      coverage: number;
    };
    bugs: BugReport[];
  }> {
    await this.logger.info('Processing QA task', { taskId: task.id }, this.agentId);

    // Generate test plan
    const testPlan = await this.generateTestPlan(task);

    // Generate test cases
    const testCases = await this.generateTestCases(task);

    // Execute tests
    const executionResults = await this.executeTests(testCases);

    // Analyze results and create bug reports
    const bugs = await this.analyzeBugs(executionResults.failed_cases, task.id);

    const results = {
      total: testCases.length,
      passed: executionResults.passed,
      failed: executionResults.failed,
      coverage: this.calculateCoverage(testCases),
    };

    await this.logger.info('QA testing completed', { results, bugCount: bugs.length }, this.agentId);

    return {
      test_plan: testPlan,
      test_cases: testCases,
      test_results: results,
      bugs,
    };
  }

  /**
   * Generate test plan
   */
  private async generateTestPlan(task: Task): Promise<string> {
    const testPlan = `# Test Plan: ${task.title}

## Scope
Testing for: ${task.description}

## Test Types
1. **Functional Testing**: Verify all functional requirements
2. **Integration Testing**: Test component interactions
3. **Performance Testing**: Verify response times
4. **Security Testing**: Check for vulnerabilities
5. **Usability Testing**: Verify user experience

## Test Environment
- Environment: Cloudflare Workers (staging)
- Database: D1 (test instance)
- Test data: Synthetic data set

## Entry Criteria
- [ ] Feature implementation complete
- [ ] Code review passed
- [ ] Unit tests pass
- [ ] Deployment to staging successful

## Exit Criteria
- [ ] All test cases executed
- [ ] Critical and high bugs resolved
- [ ] Test coverage > 80%
- [ ] Performance targets met

## Test Schedule
- Test preparation: 1 day
- Test execution: 2 days
- Bug fixing and retest: 1-2 days

## Risks
- Incomplete requirements
- Environment issues
- Time constraints

---
*Generated by QA Agent for Task ${task.id}*
`;

    return testPlan;
  }

  /**
   * Generate test cases
   */
  private async generateTestCases(task: Task): Promise<TestCase[]> {
    const testCases: TestCase[] = [];

    // Functional test cases
    testCases.push({
      id: `tc-${Date.now()}-1`,
      title: 'Verify core functionality works as expected',
      description: `Test that ${task.description} functions correctly`,
      pre_conditions: ['User is authenticated', 'System is operational'],
      steps: ['Navigate to feature', 'Execute main action', 'Verify result'],
      expected_result: 'Feature works as specified in PRD',
      status: 'pending',
      severity: 'critical',
      created_at: Date.now(),
    });

    // Error handling test cases
    testCases.push({
      id: `tc-${Date.now()}-2`,
      title: 'Verify error handling with invalid input',
      description: 'Test system behavior with invalid data',
      pre_conditions: ['User is authenticated'],
      steps: ['Submit invalid input', 'Check error message', 'Verify system stability'],
      expected_result: 'Clear error message displayed, system remains stable',
      status: 'pending',
      severity: 'high',
      created_at: Date.now(),
    });

    // Performance test cases
    testCases.push({
      id: `tc-${Date.now()}-3`,
      title: 'Verify response time meets performance targets',
      description: 'Test API response time',
      pre_conditions: ['Feature deployed to staging'],
      steps: ['Send API request', 'Measure response time', 'Compare with target'],
      expected_result: 'Response time < 500ms',
      status: 'pending',
      severity: 'medium',
      created_at: Date.now(),
    });

    // Security test cases
    testCases.push({
      id: `tc-${Date.now()}-4`,
      title: 'Verify authentication and authorization',
      description: 'Test access controls',
      pre_conditions: ['Multiple user roles configured'],
      steps: ['Test with different user roles', 'Verify access permissions', 'Test unauthorized access'],
      expected_result: 'Proper access control enforced',
      status: 'pending',
      severity: 'critical',
      created_at: Date.now(),
    });

    // Integration test cases
    testCases.push({
      id: `tc-${Date.now()}-5`,
      title: 'Verify integration with other components',
      description: 'Test component interactions',
      pre_conditions: ['All components deployed'],
      steps: [
        'Trigger interaction between components',
        'Verify data flow',
        'Check for integration errors',
      ],
      expected_result: 'Components interact correctly',
      status: 'pending',
      severity: 'high',
      created_at: Date.now(),
    });

    return testCases;
  }

  /**
   * Execute test cases
   */
  private async executeTests(testCases: TestCase[]): Promise<{
    passed: number;
    failed: number;
    failed_cases: TestCase[];
  }> {
    let passed = 0;
    let failed = 0;
    const failedCases: TestCase[] = [];

    for (const testCase of testCases) {
      // In production, this would actually execute the tests
      // For now, simulate test execution
      const success = Math.random() > 0.2; // 80% pass rate simulation

      if (success) {
        testCase.status = 'passed';
        testCase.actual_result = testCase.expected_result;
        passed++;
      } else {
        testCase.status = 'failed';
        testCase.actual_result = 'Unexpected behavior detected';
        failed++;
        failedCases.push(testCase);
      }

      await this.logger.info(`Test case ${testCase.id}: ${testCase.status}`, { testCase }, this.agentId);
    }

    return { passed, failed, failed_cases: failedCases };
  }

  /**
   * Analyze failed tests and create bug reports
   */
  private async analyzeBugs(failedCases: TestCase[], taskId: string): Promise<BugReport[]> {
    const bugs: BugReport[] = [];

    for (const testCase of failedCases) {
      const bug: BugReport = {
        id: `bug-${Date.now()}-${bugs.length}`,
        title: `Bug in: ${testCase.title}`,
        severity: testCase.severity,
        steps_to_reproduce: testCase.steps,
        expected_behavior: testCase.expected_result,
        actual_behavior: testCase.actual_result || 'Unexpected behavior',
        environment: `Cloudflare Workers (staging)`,
        task_id: taskId,
        created_at: Date.now(),
      };

      bugs.push(bug);

      // Log bug to database
      await this.env.DB.prepare(
        `INSERT INTO system_logs (id, level, component, message, details, agent_id, task_id, created_at)
         VALUES (?, ?, ?, ?, ?, ?, ?, ?)`
      )
        .bind(
          bug.id,
          'error',
          'QA',
          bug.title,
          JSON.stringify(bug),
          this.agentId,
          taskId,
          Date.now()
        )
        .run();
    }

    return bugs;
  }

  /**
   * Calculate test coverage
   */
  private calculateCoverage(testCases: TestCase[]): number {
    // In production, this would analyze code coverage
    // For now, estimate based on test case diversity
    const testTypes = new Set(testCases.map((tc) => tc.severity));
    return Math.min((testTypes.size / 4) * 100, 100); // Max 100%
  }

  /**
   * Generate test report
   */
  async generateTestReport(taskId: string, results: {
    test_plan: string;
    test_cases: TestCase[];
    test_results: {
      total: number;
      passed: number;
      failed: number;
      coverage: number;
    };
    bugs: BugReport[];
  }): Promise<string> {
    const report = `# QA Test Report

## Task: ${taskId}

## Executive Summary
- **Total Test Cases**: ${results.test_results.total}
- **Passed**: ${results.test_results.passed}
- **Failed**: ${results.test_results.failed}
- **Pass Rate**: ${((results.test_results.passed / results.test_results.total) * 100).toFixed(1)}%
- **Coverage**: ${results.test_results.coverage.toFixed(1)}%

## Test Execution Status
${results.test_cases
  .map(
    (tc) =>
      `- [${tc.status === 'passed' ? 'x' : ' '}] ${tc.title} (${tc.severity}) - ${tc.status.toUpperCase()}`
  )
  .join('\n')}

## Bugs Found
${
  results.bugs.length > 0
    ? results.bugs
        .map(
          (bug) => `
### ${bug.title} (${bug.severity.toUpperCase()})
- **Steps to Reproduce**: ${bug.steps_to_reproduce.join(' → ')}
- **Expected**: ${bug.expected_behavior}
- **Actual**: ${bug.actual_behavior}
`
        )
        .join('\n')
    : 'No bugs found ✅'
}

## Recommendations
${
  results.test_results.failed > 0
    ? `- Fix ${results.bugs.filter((b) => b.severity === 'critical').length} critical bugs before deployment
- Address ${results.bugs.filter((b) => b.severity === 'high').length} high priority bugs
- Schedule retest after bug fixes`
    : '- All tests passed! Ready for deployment ✅'
}

---
*Generated by QA Agent at ${new Date().toISOString()}*
`;

    return report;
  }

  /**
   * Perform regression testing
   */
  async performRegressionTest(previousTestCases: TestCase[]): Promise<{
    regression_detected: boolean;
    affected_cases: TestCase[];
  }> {
    const affectedCases: TestCase[] = [];

    for (const testCase of previousTestCases) {
      if (testCase.status === 'passed') {
        // Re-run previously passed tests
        const stillPasses = Math.random() > 0.1; // 90% stability
        if (!stillPasses) {
          testCase.status = 'failed';
          testCase.actual_result = 'Regression detected - previously working feature now fails';
          affectedCases.push(testCase);
        }
      }
    }

    return {
      regression_detected: affectedCases.length > 0,
      affected_cases: affectedCases,
    };
  }
}
